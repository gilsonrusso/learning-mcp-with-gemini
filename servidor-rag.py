import os

import dotenv
from fastmcp import FastMCP

# RAG
from langchain_ollama import OllamaEmbeddings
from langchain_qdrant import QdrantVectorStore
from qdrant_client import QdrantClient

dotenv.load_dotenv()

MODEL_EMBEDDING = os.getenv("MODEL_EMBEDDING", "nomic-embed-text:v1.5")
URL_QDRANT = os.getenv("URL_QDRANT", "http://localhost:6333")
NOME_COLECAO = os.getenv("NOME_COLECAO", "documentacao_teste")

# Configura o modelo de embeddings (o mesmo que usamos para salvar)
embeddings = OllamaEmbeddings(model=MODEL_EMBEDDING)

# Conecta ao Qdrant que est√° rodando no Docker
qdrant_client = QdrantClient(url=URL_QDRANT)

# Cria o Vector Store (a "mem√≥ria")
vector_store = QdrantVectorStore(
    client=qdrant_client,  # Cliente do Qdrant
    collection_name=NOME_COLECAO,  # Nome da cole√ß√£o (tabela)
    embedding=embeddings,  # Modelo de embeddings
)


# ==========================================
# Criando o Servidor FastMCP para o RAG
# ==========================================
mcp = FastMCP(
    name="RAG Documenta√ß√£o",
    instructions="Servidor especializado em buscar informa√ß√µes na base de conhecimento interna.",
)


@mcp.tool()
def buscar_documentacao(duvida: str) -> str:
    """
    Busca informa√ß√µes na base de conhecimento vetorial (RAG).
    A base de dados cont√©m documentos variados, incluindo hist√≥rias infantis.
    Use esta ferramenta SEMPRE que o usu√°rio fizer uma pergunta sobre hist√≥rias, personagens, ou necessitar de uma busca na base de dados de textos.
    """
    print(f"\n[MCP Server RAG] üîç Buscando no RAG por: '{duvida}'")

    # Faz a busca vetorial aumentando K para retornar os 6 melhores matches
    resultados = vector_store.similarity_search(duvida, k=6)

    if not resultados:
        return "Nenhuma informa√ß√£o relevante foi encontrada na documenta√ß√£o."

    # Formata os peda√ßos encontrados em um texto √∫nico para o LLM ler
    textos = [f"- {doc.page_content}" for doc in resultados]
    resposta_formatada = (
        "Trechos relevantes encontrados na documenta√ß√£o:\n" + "\n".join(textos)
    )

    return resposta_formatada


@mcp.resource("system://rag_manager")
def rag_manager():
    """Instru√ß√µes para o assistente utilizar a base de dados."""
    return (
        "Voc√™ √© um assistente especialista na base de conhecimentos vetorial (RAG). "
        "Siga estas diretrizes ao responder:\n"
        "1. A base de dados pode conter informa√ß√µes variadas, como hist√≥rias, contos ou dados de conhecimento gerais.\n"
        "2. Baseie-se e confie nas informa√ß√µes retornadas pela ferramenta `buscar_documentacao` para responder.\n"
        "3. A resposta DEVE ser formatada obrigatoriamente em Markdown para melhor leitura.\n"
    )


if __name__ == "__main__":
    # Roda o RAG na porta 8001 para n√£o conflitar com a Pet Store na porta 8000
    mcp.run(transport="http", port=8000)
